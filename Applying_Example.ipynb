{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#load data\n",
    "data = pd.read_csv('housing.csv')\n",
    "print(data.head(5))\n",
    "target_attr = 'median_house_value'\n",
    "#Data preprocessing\n",
    "#dirty data cleaning\n",
    "#None data in 'total_bedrooms' attr -> drop\n",
    "#20640 rows -> 20433 rows\n",
    "original_length = len(data)\n",
    "data = data.dropna(axis = 0)\n",
    "print('number of deleted data : ',original_length,'->',len(data))\n",
    "#feature engineering\n",
    "#draw heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (16,5))\n",
    "\n",
    "cor_matrix = data.corr()\n",
    "ax = sns.heatmap(cor_matrix, annot = True, fmt = \".2f\")\n",
    "plt.title('HeatMap', fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "print('correlation with target_attr : best to worst')\n",
    "cor_sorted = cor_matrix[target_attr].drop(target_attr).sort_values(key = abs, ascending = False)\n",
    "print(cor_sorted)\n",
    "# Attr with correlation(- 0.05 ~ + 0.05) can be dismissed -> Attr : total_bedrooms, longitude, population\n",
    "#drop 3 attr\n",
    "data = data.drop(['total_bedrooms','longitude','population'],axis = 1)\n",
    "data = data.reset_index(drop= True)\n",
    "#divide dataset to non_target and target\n",
    "dataset_non_target = data.drop([target_attr], axis = 1, inplace = False)\n",
    "dataset_target = pd.DataFrame(data[target_attr], columns = [target_attr])\n",
    "#scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "#encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "#train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import mixture\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "#score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import math\n",
    "#visualize\n",
    "from sklearn.decomposition import PCA\n",
    "def AutoML(scaler_list, encoder_list, model_list, hyperparmeter_df, dataset_no_target, dataset_target, categorical_attr_list, measure_df):\n",
    "    \n",
    "    function_result_df = pd.DataFrame(columns = ['scaling_type','encoding_type','model_name','algorithm_parameters','knee_method','silhouette_score','purity','dist'])\n",
    "    function_result_cnt = 0\n",
    "    \n",
    "    answer_df = pd.DataFrame(columns = ['k','answer'])\n",
    "    sorted_dt = dataset_target.sort_values(by = [target_attr])#datset for make answer after\n",
    "    sorted_dt['index'] = sorted_dt.index\n",
    "    sorted_dt = sorted_dt.reset_index(drop = True)\n",
    "\n",
    "    for k in range(2, 13):#arbitrarily k range\n",
    "        first_idx = int(len(sorted_dt) / k)\n",
    "\n",
    "        split_val_list = []\n",
    "        #split to cluster and store each cluster's original dataset index\n",
    "        temp = sorted_dt\n",
    "        for i in range(1, k):#because there is multiplication, we use range start at 1 to k.\n",
    "            idx = first_idx * i\n",
    "            split_val = sorted_dt.iloc[idx, 0]\n",
    "            split_val_list.append(temp[temp[target_attr] <= split_val]['index'].sort_values())\n",
    "            temp = temp[temp[target_attr] > split_val]\n",
    "        split_val_list.append(temp['index'].sort_values())\n",
    "        \n",
    "        #calculate all cluster answer and store it to cluster_answer dataframe\n",
    "        cluster_answer = pd.DataFrame()\n",
    "        for i in range(len(split_val_list)):\n",
    "            each_cluster = pd.DataFrame(index = split_val_list[i], columns = ['cluster'])\n",
    "            each_cluster = each_cluster.fillna(i)\n",
    "            cluster_answer = pd.concat([cluster_answer, each_cluster])\n",
    "        \n",
    "            cluster_answer = cluster_answer.sort_index()\n",
    "        #store all it to answer_df\n",
    "        answer_df.loc[len(answer_df)] = [k, cluster_answer['cluster'].to_list()]\n",
    "    #Separate dataset to be scaled and dataset to be encoded\n",
    "    if categorical_attr_list == ['none'] or len(categorical_attr_list) == 0:#there is no categorical attribute in dataset\n",
    "        dt_bf_scaling = dataset_no_target\n",
    "        dt_bf_encoding = None\n",
    "    else:#there is some categorical attribute in dataset\n",
    "        dt_bf_scaling = dataset_no_target.drop(categorical_attr_list, axis = 1, inplace = False)\n",
    "        dt_bf_scaling = dt_bf_scaling.reset_index(drop= True)\n",
    "        dt_bf_encoding = dataset_no_target[categorical_attr_list]\n",
    "\n",
    "    #scaling\n",
    "    for scaling_type in scaler_list:\n",
    "        dt_after_scaling = None\n",
    "        # 6 type of scaling : none, standard, minmax, amxabs, robust, norm\n",
    "        if scaling_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling_type == 'maxabs':\n",
    "            scaler = MaxAbsScaler()\n",
    "        elif scaling_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        elif scaling_type == 'norm':\n",
    "            scaler = Normalizer()\n",
    "        else:\n",
    "            print('ERROR : scaling type error')\n",
    "            return\n",
    "        \n",
    "        scaled_data = scaler.fit_transform(dt_bf_scaling)\n",
    "        dt_after_scaling = pd.DataFrame(scaled_data, columns = dt_bf_scaling.columns)\n",
    " \n",
    "        #encoding\n",
    "        #there is no encoding data\n",
    "        for encoding_type in encoder_list:\n",
    "            dt_after_encoding = None\n",
    "            if dt_bf_encoding.empty :#there is no data to encoding\n",
    "                pass\n",
    "            #3 type of encoding : label, onehot, ordinal\n",
    "            if encoding_type == 'label':\n",
    "                encoder = LabelEncoder()\n",
    "                    \n",
    "                for categorical_attr in categorical_attr_list:\n",
    "                    encoded_data = encoder.fit_transform(dt_bf_encoding[categorical_attr])\n",
    "                    encoded_data_df = pd.DataFrame(encoded_data, columns = [categorical_attr])\n",
    "                    dt_after_encoding = pd.concat([dt_after_encoding, encoded_data_df], axis = 1)\n",
    "            elif encoding_type == 'onehot':\n",
    "                dt_after_encoding = pd.get_dummies(dt_bf_encoding)\n",
    "            elif encoding_type == 'ordinal':\n",
    "                encoder = OrdinalEncoder()\n",
    "               \n",
    "                for categorical_attr in categorical_attr_list:\n",
    "                    encoded_data = encoder.fit_transform(dt_bf_encoding[categorical_attr].values.reshape(-1, 1))\n",
    "                    encoded_data_df = pd.DataFrame(encoded_data, columns = [categorical_attr])\n",
    "                    dt_after_encoding = pd.concat([dt_after_encoding, encoded_data_df], axis = 1)\n",
    "            else:\n",
    "                print('ERROR : encoder type error')\n",
    "                return  \n",
    "\n",
    "            #merge dataset to be scaled and dataset to be encoded\n",
    "            dataset_no_target = pd.concat([dt_after_scaling,dt_after_encoding], axis = 1)\n",
    "\n",
    "            #make model\n",
    "            model_cnt = 0\n",
    "            for model_name in model_list:\n",
    "                params = hyperparmeter_df.iloc[:, model_cnt][0]\n",
    "                hyperparameter_dict = eval(str(params))#eval : function to string -> dict reference : https://blog.metafor.kr/224\n",
    "                measure = measure_df.iloc[:,model_cnt][0]\n",
    "                model_cnt = model_cnt + 1\n",
    "                if model_name == 'K-means': \n",
    "                    if 'n_clusters' in hyperparameter_dict:\n",
    "                        k_params = hyperparameter_dict['n_clusters']\n",
    "                    else : \n",
    "                        k_params = 5\n",
    "                    if 'algorithm' in hyperparameter_dict:\n",
    "                        algorithm_params = hyperparameter_dict['algorithm']\n",
    "                    else : \n",
    "                        algorithm_params = ['lloyd']\n",
    "                    for algorithm_param in algorithm_params:    \n",
    "                        for k_param in k_params:\n",
    "                            model = KMeans(n_clusters = k_param, algorithm = algorithm_param)\n",
    "                                \n",
    "                            y_pred = model.fit_predict(dataset_no_target)\n",
    "                            y_true = answer_df[answer_df['k'] == k_param]['answer'].to_list()\n",
    "\n",
    "                            #do pca(change XD to 2D) and visualize(2D)\n",
    "                            params = 'scaling : ' + scaling_type + '/encoding : ' + encoding_type + '/k_param : ' + str(k_param) + '/algorithm : ' + str(algorithm_param)\n",
    "                            pca_visualize(dataset_no_target,y_pred,k_param,model_name,params)\n",
    "                            \n",
    "                            knee_method_result = None\n",
    "                            silhouette_score_result = None\n",
    "                            purity_result = None\n",
    "                            if 'knee-method' in measure: \n",
    "                                knee_method_result = model.inertia_\n",
    "                                #print('knee method', model.inertia_)\n",
    "                            if 'silhouette_score' in measure:\n",
    "                                silhouette_score_result = silhouette_score(dataset_no_target, y_pred)\n",
    "                                #print('silhouette_score',silhouette_score_result)\n",
    "                            if 'purity' in measure:\n",
    "                                contingency_matrix = metrics.cluster.contingency_matrix(y_true,y_pred)\n",
    "                                purity_result = np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix)\n",
    "                                #print('purity',np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix))\n",
    "        \n",
    "                            original_center = cal_cluster_mean(dataset_no_target,answer_df, k_param)\n",
    "                            dist_list = []\n",
    "                            for i in range(0, k_param):\n",
    "                                dist = math.dist(model.cluster_centers_[i], original_center[i])\n",
    "                                dist_list.append(dist)\n",
    "                            final_dst = sum(dist_list) / k_param\n",
    "                            #print('avg_dist_of_centers',sum(dist_list) / k_param)\n",
    "                            \n",
    "                            params = 'k_param : ' + str(k_param) + ' algorithm : ' + str(algorithm_param)\n",
    "                            result_list = make_result_list(scaling_type, encoding_type, model_name, params, \n",
    "                                                           knee_method_result, silhouette_score_result, purity_result, sum(dist_list) / k_param)\n",
    "                            function_result_df.loc[function_result_cnt]=result_list\n",
    "                            function_result_cnt = function_result_cnt + 1               \n",
    "                elif model_name == 'EM':\n",
    "                    if 'n_component' in hyperparameter_dict:\n",
    "                        n_params = hyperparameter_dict['n_component']\n",
    "                    else : \n",
    "                        n_params = range(5,6)\n",
    "                    if 'covariance_type' in hyperparameter_dict:\n",
    "                        covar_types = hyperparameter_dict['n_component']\n",
    "                    else :\n",
    "                        covar_types = ['full']\n",
    "                    for type_param in hyperparameter_dict['covariance_type']:    \n",
    "                        for n_param in hyperparameter_dict['n_component']:\n",
    "                            model = mixture.GaussianMixture(n_components = n_param, covariance_type = type_param)\n",
    "                                    \n",
    "                            y_true = answer_df[answer_df['k'] == n_param]['answer'].to_list()\n",
    "                            y_pred = model.fit_predict(dataset_no_target)\n",
    "                            \n",
    "                            #do pca(change XD to 2D) and visualize(2D)\n",
    "                            params = 'scaling : ' + scaling_type + '/encoding : ' + encoding_type  + '/n_component : ' + str(n_param) + '/covariance_type : ' + str(type_param)\n",
    "                            pca_visualize(dataset_no_target,y_pred,n_param,model_name,params)  \n",
    "                                \n",
    "                            knee_method_result = None\n",
    "                            silhouette_score_result = None\n",
    "                            purity_result = None\n",
    "                            if 'silhouette_score' in measure:\n",
    "                                silhouette_score_result = silhouette_score(dataset_no_target, y_pred)\n",
    "                                #print('silhouette_score',silhouette_score_result)\n",
    "                            if 'purity' in measure:\n",
    "                                contingency_matrix = metrics.cluster.contingency_matrix(y_true,y_pred)\n",
    "                                purity_result = np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix)\n",
    "                                #print('purity',np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix))\n",
    "                                \n",
    "                            original_center = cal_cluster_mean(dataset_no_target,answer_df, n_param)\n",
    "                            dist_list = []\n",
    "                            for i in range(0, n_param):\n",
    "                                dist = math.dist(model.means_[i], original_center[i])\n",
    "                                dist_list.append(dist)\n",
    "                            final_dst = sum(dist_list) / k_param\n",
    "                            #print('avg_dist_of_centers',sum(dist_list) / n_param)\n",
    "                            \n",
    "                            params = 'n_component : ' + str(n_param) + ' covariance_type : ' + str(type_param)\n",
    "                            result_list = make_result_list(scaling_type, encoding_type, model_name, params, \n",
    "                                                           knee_method_result, silhouette_score_result, purity_result, sum(dist_list) / k_param)\n",
    "                            function_result_df.loc[function_result_cnt]=result_list\n",
    "                            function_result_cnt = function_result_cnt + 1           \n",
    "                                      \n",
    "                elif model_name == 'CLARANS':\n",
    "                    if 'number_clusters' in hyperparameter_dict:\n",
    "                        n_c_params = hyperparameter_dict['number_clusters']\n",
    "                    else :\n",
    "                        n_c_params = [3]\n",
    "                    if 'maxneighbor' in hyperparameter_dict:\n",
    "                        m_n_params = hyperparameter_dict['maxneighbor']\n",
    "                    else :\n",
    "                        m_n_params = [0]\n",
    "                    for n_c_param in n_c_params:\n",
    "                        for m_n_param in m_n_params:\n",
    "                            model  = clarans(dataset_no_target.values.tolist(), n_c_param, 2, m_n_param)\n",
    "                            model.process()\n",
    "                            cluster_index = model.get_clusters()\n",
    "\n",
    "                            cluster_answer = pd.DataFrame()\n",
    "                            for i in range(n_c_param):\n",
    "                                each_cluster = pd.DataFrame(index = cluster_index[i], columns = ['cluster'])\n",
    "                                each_cluster = each_cluster.fillna(i)\n",
    "                                cluster_answer = pd.concat([cluster_answer, each_cluster])\n",
    "        \n",
    "                            cluster_answer = cluster_answer.sort_index()\n",
    "                            y_pred = cluster_answer['cluster'].to_list()\n",
    "                            y_true = answer_df[answer_df['k'] == n_c_param]['answer'].to_list()\n",
    "                    \n",
    "                            #do pca(change XD to 2D) and visualize(2D)\n",
    "                            params = 'scaling : ' + scaling_type + '/encoding : ' + encoding_type + '/number_clusters : ' + str(n_c_param) + '/maxneighbor : ' + str(m_n_param)\n",
    "                            pca_visualize(dataset_no_target,y_pred,n_c_param,model_name,params)\n",
    "                            \n",
    "                            knee_method_result = None\n",
    "                            silhouette_score_result = None\n",
    "                            purity_result = None\n",
    "                            if 'silhouette_score' in measure:\n",
    "                                silhouette_score_result = silhouette_score(dataset_no_target, y_pred)\n",
    "                                #print('silhouette_score',silhouette_score_result)\n",
    "                            if 'purity' in measure:\n",
    "                                contingency_matrix = metrics.cluster.contingency_matrix(y_true,y_pred)\n",
    "                                purity_result = np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix)\n",
    "                                #print('purity',np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix))\n",
    "                                \n",
    "                            original_center = cal_cluster_mean(dataset_no_target,answer_df, n_c_param)\n",
    "                            dist_list = []\n",
    "                            for i in range(n_c_param):\n",
    "                                dist = math.dist(dataset_no_target.iloc[model.get_medoids()[i]].to_list(), original_center[i])\n",
    "                                dist_list.append(dist)\n",
    "                            final_dst = sum(dist_list) / k_param\n",
    "                            #print('avg_dist_of_centers',sum(dist_list) / n_c_param)\n",
    "                            \n",
    "                            params = 'number_clusters : ' + str(n_c_param) + ' maxneighbor : ' + str(m_n_param)\n",
    "                            result_list = make_result_list(scaling_type, encoding_type, model_name, params, \n",
    "                                                           knee_method_result, silhouette_score_result, purity_result, sum(dist_list) / k_param)\n",
    "                            function_result_df.loc[function_result_cnt]=result_list\n",
    "                            function_result_cnt = function_result_cnt + 1   \n",
    "                                    \n",
    "                elif model_name == 'DBSCAN':\n",
    "                    if 'eps' in hyperparameter_dict:\n",
    "                        eps_params = hyperparameter_dict['eps']\n",
    "                    else :\n",
    "                        eps_params = [0.5]\n",
    "                    if 'min_samples' in hyperparameter_dict:\n",
    "                        min_sample_params = hyperparameter_dict['min_samples']\n",
    "                    else :\n",
    "                        min_sample_params = [2]\n",
    "                    for eps_param in eps_params:\n",
    "                        for min_sample_param in min_sample_params:\n",
    "                            model = DBSCAN(eps = eps_param, min_samples = min_sample_param)\n",
    "                            \n",
    "                            y_pred = model.fit_predict(dataset_no_target)\n",
    "                            if scaling_type == 'standard':\n",
    "                                y_pred = y_pred + 1\n",
    "                            \n",
    "                            k_val = y_pred.max()\n",
    "                            if k_val > 12 or k_val < 2:\n",
    "                                if k_val > 12:\n",
    "                                    knee_method_result = None\n",
    "                                    silhouette_score_result = None\n",
    "                                    purity_result = None\n",
    "                                    if 'silhouette_score' in measure:\n",
    "                                        silhouette_score_result = silhouette_score(dataset_no_target, y_pred)\n",
    "                                    \n",
    "                                    params = 'eps_param : ' + str(eps_param) + ' min_samples : ' + str(min_sample_param)\n",
    "                                    result_list = make_result_list(scaling_type, encoding_type, model_name, params, \n",
    "                                                           knee_method_result, silhouette_score_result, purity_result, sum(dist_list) / k_param)\n",
    "                                    function_result_df.loc[function_result_cnt]=result_list\n",
    "                                    function_result_cnt = function_result_cnt + 1  \n",
    "                            else:\n",
    "                                y_true = answer_df[answer_df['k'] == k_val]['answer'].to_list()\n",
    "                                #do pca(change XD to 2D) and visualize(2D)\n",
    "                                params = 'scaling : ' + scaling_type + '/encoding : ' + encoding_type  + '/eps_param : ' + str(eps_param) + '/min_samples : ' + str(min_sample_param)\n",
    "                                pca_visualize(dataset_no_target,y_pred,k_val,model_name,params)  \n",
    "                            \n",
    "                                knee_method_result = None\n",
    "                                silhouette_score_result = None\n",
    "                                purity_result = None\n",
    "                                if 'silhouette_score' in measure:\n",
    "                                    silhouette_score_result = silhouette_score(dataset_no_target, y_pred)\n",
    "                                    #print('silhouette_score',silhouette_score_result)\n",
    "                                if 'purity' in measure:\n",
    "                                    contingency_matrix = metrics.cluster.contingency_matrix(y_true,y_pred)\n",
    "                                    purity_result = np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix)\n",
    "                                    #print('purity',purity_result)\n",
    "                                \n",
    "                                original_center = cal_cluster_mean(dataset_no_target,answer_df, k_val)\n",
    "                                temp = pd.concat([dataset_no_target, pd.DataFrame(y_pred.tolist(), columns = ['cluster'])], axis = 1)\n",
    "                                dist_list = []\n",
    "                                for i in range(0, k_val):\n",
    "                                    dist = math.dist(temp.groupby('cluster').mean().values.tolist()[i], original_center[i])\n",
    "                                    dist_list.append(dist)\n",
    "                                final_dst = sum(dist_list) / k_param\n",
    "                                #print('avg_dist_of_centers',sum(dist_list) / k_val)\n",
    "                                \n",
    "                                params = 'eps_param : ' + str(eps_param) + ' min_samples : ' + str(min_sample_param)\n",
    "                                result_list = make_result_list(scaling_type, encoding_type, model_name, params, \n",
    "                                                           knee_method_result, silhouette_score_result, purity_result, sum(dist_list) / k_param)\n",
    "                                function_result_df.loc[function_result_cnt]=result_list\n",
    "                                function_result_cnt = function_result_cnt + 1   \n",
    "                                 \n",
    "                elif model_name == 'AffinityPropagation':\n",
    "                    if 'preference' in hyperparameter_dict:\n",
    "                        preference_params = hyperparameter_dict['preference']\n",
    "                    else :\n",
    "                        preference_params = [15]\n",
    "                    if 'max_iter' in hyperparameter_dict:\n",
    "                        max_iter_params = hyperparameter_dict['max_iter']\n",
    "                    else :\n",
    "                        max_iter_params = [200]\n",
    "                    for preference_param in preference_params:\n",
    "                        for max_iter_param in max_iter_params:\n",
    "                            model = AffinityPropagation(damping = 0.99, preference = preference_param, max_iter = max_iter_param)\n",
    "                            \n",
    "                            y_pred = model.fit_predict(dataset_no_target)\n",
    "                            k_val = y_pred.max() + 1\n",
    "                            \n",
    "                            if k_val > 12 or k_val < 2:\n",
    "                                if k_val > 12:\n",
    "                                    knee_method_result = None\n",
    "                                    silhouette_score_result = None\n",
    "                                    purity_result = None\n",
    "                                    if 'silhouette_score' in measure:\n",
    "                                        silhouette_score_result = silhouette_score(dataset_no_target, y_pred)\n",
    "                                \n",
    "                                    params = 'preference_param : ' + str(preference_param) + ' max_iter : ' + str(max_iter_param)\n",
    "                                    result_list = make_result_list(scaling_type, encoding_type, model_name, params, \n",
    "                                                           knee_method_result, silhouette_score_result, purity_result, sum(dist_list) / k_param)\n",
    "                                    function_result_df.loc[function_result_cnt]=result_list\n",
    "                                    function_result_cnt = function_result_cnt + 1   \n",
    "                            else:\n",
    "                                y_true = answer_df[answer_df['k'] == k_val]['answer'].to_list()\n",
    "                            \n",
    "                                #do pca(change XD to 2D) and visualize(2D)\n",
    "                                params = 'scaling : ' + scaling_type + '/encoding : ' + encoding_type  + '/preference_param : ' + str(preference_param) + '/max_iter : ' + str(max_iter_param)\n",
    "                                pca_visualize(dataset_no_target,y_pred,k_val,model_name,params)   \n",
    "                            \n",
    "                                knee_method_result = None\n",
    "                                silhouette_score_result = None\n",
    "                                purity_result = None\n",
    "                                if 'silhouette_score' in measure:\n",
    "                                    silhouette_score_result = silhouette_score(dataset_no_target, y_pred)\n",
    "                                    #print('silhouette_score',silhouette_score_result)\n",
    "                                if 'purity' in measure:\n",
    "                                    contingency_matrix = metrics.cluster.contingency_matrix(y_true,y_pred)\n",
    "                                    purity_result = np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix)\n",
    "                                    #print('purity',np.sum(np.amax(contingency_matrix, axis=0)) /  np.sum(contingency_matrix))\n",
    "                                \n",
    "                                original_center = cal_cluster_mean(dataset_no_target,answer_df, k_val)\n",
    "                                dist_list = []\n",
    "                                for i in range(0, k_val):\n",
    "                                    dist = math.dist(model.cluster_centers_[i], original_center[i])\n",
    "                                    dist_list.append(dist)\n",
    "                                final_dst = sum(dist_list) / k_param\n",
    "                                #print('avg_dist_of_centers',sum(dist_list) / k_val)\n",
    "                                \n",
    "                                params = 'preference_param : ' + str(preference_param) + ' max_iter : ' + str(max_iter_param)\n",
    "                                result_list = make_result_list(scaling_type, encoding_type, model_name, params, \n",
    "                                                           knee_method_result, silhouette_score_result, purity_result, sum(dist_list) / k_param)\n",
    "                                function_result_df.loc[function_result_cnt]=result_list\n",
    "                                function_result_cnt = function_result_cnt + 1              \n",
    "                else:\n",
    "                    print('ERROR : Model name error')\n",
    "                    return\n",
    "    return function_result_df\n",
    "def cal_cluster_mean(dt_n_t, answer_df, k):\n",
    "    train_answer_df = pd.concat([dt_n_t, pd.DataFrame(answer_df[answer_df['k'] == k]['answer'][k - 2], columns = ['answer'])], axis = 1)\n",
    "    \n",
    "    cluster_mean_list = []\n",
    "    for i in range(0, k):\n",
    "        temp_c = train_answer_df[train_answer_df['answer'] == i]\n",
    "        temp_c_mean = temp_c.mean().to_list()\n",
    "        del temp_c_mean[-1]\n",
    "        cluster_mean_list.append(temp_c_mean)\n",
    "    return cluster_mean_list\n",
    "def pca_visualize(dataset_no_target, y_pred,k, model_name,params):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax2 = fig.add_subplot(111)\n",
    "    cluster_colors = ['#FF9999', '#FFCC99', '#FFFF99', '#CCFF99', '#99FF99', '#99FFCC','#99FFFF', '#99CCFF', '#9999FF', \n",
    "                      '#CC99FF', '#FF99FF', '#FF99CC','#FFFFFF','#BE3559']\n",
    "    color_index = 0\n",
    "    temp = pd.concat([dataset_no_target, pd.DataFrame(y_pred, columns = ['pred'])], axis = 1)\n",
    "    pca_2 = PCA(n_components=2,random_state = 42)\n",
    "    for i in range(0, k):\n",
    "        df_bf_pca = temp[temp['pred'] == i]\n",
    "        if len(df_bf_pca) == 1:\n",
    "            temp_list = df_bf_pca.values.tolist()\n",
    "            temp_list = max(temp_list)\n",
    "            ax2.scatter(np.mean(temp_list),np.mean(temp_list),color=cluster_colors[i],alpha=0.5,marker='o')\n",
    "        else:\n",
    "            df_af_pca_2 = pca_2.fit_transform(df_bf_pca)\n",
    "            ax2.scatter(df_af_pca_2[:,0],df_af_pca_2[:,1],color=cluster_colors[i],alpha=0.5,marker='o')\n",
    "    temp_string = model_name, params\n",
    "    ax2.set_title(temp_string)\n",
    "    labels = [\"cluster \"+str(k) for k in range(k)]\n",
    "    fig.legend(labels, loc='lower center',ncol=len(labels), bbox_transform=(1,0),borderaxespad=-0.5)\n",
    "    plt.show()\n",
    "#'scaling_type','encoding_type','model_name','algorithm_parameters','measures'\n",
    "def make_result_list(scaling_type, encoding_type, model_name, parameters, knee, score1, score2 , dist):\n",
    "    result_list = []\n",
    "    result_list.append(scaling_type)\n",
    "    result_list.append(encoding_type)\n",
    "    result_list.append(model_name)\n",
    "    result_list.append(parameters)\n",
    "    result_list.append(knee)\n",
    "    result_list.append(score1)\n",
    "    result_list.append(score2)\n",
    "    result_list.append(dist)\n",
    "    return result_list  \n",
    "scaler_list = ['standard','minmax','maxabs','robust','norm']#5 scaler :none sacling\n",
    "encoder_list = ['label', 'onehot', 'ordinal']\n",
    "model_list = ['K-means','EM','CLARANS','DBSCAN','AffinityPropagation']#5 model\n",
    "\n",
    "hyperparmeter_df = pd.DataFrame(columns = model_list)\n",
    "hyperparmeter_df.loc[0] = [0 for i in range(len(model_list))]                      \n",
    "hyperparmeter_df.iloc[:,0] = '{\\'n_clusters\\' : range(3, 7), \\'algorithm\\' : [\\'lloyd\\', \\'elkan\\']}' #first model(K-means)'s hyperparamter\n",
    "hyperparmeter_df.iloc[:,1] = '{\\'n_component\\' : range(2, 4), \\'covariance_type\\' : [\\'full\\', \\'tied\\']}'\n",
    "hyperparmeter_df.iloc[:,2] = '{\\'number_clusters\\' : range(3, 4), \\'maxneighbor\\' : [1, 0]}'\n",
    "hyperparmeter_df.iloc[:,3] = '{\\'eps\\' : [0.5], \\'min_samples\\' : [9, 10]}'\n",
    "hyperparmeter_df.iloc[:,4] = '{\\'preference\\' : [-10], \\'max_iter\\' : [10, 20]}'\n",
    "\n",
    "categorical_attr_list = ['ocean_proximity']# ['a','b'] // 'none' //not declared\n",
    "\n",
    "measure_df = pd.DataFrame(columns = model_list)\n",
    "measure_df.loc[0] = [0 for i in range(len(model_list))]                     \n",
    "measure_df.iloc[:,0] = 'knee-method, purity, silhouette_score' #first model(K-means)'s hyperparamter\n",
    "measure_df.iloc[:,1] = 'purity, silhouette_score'\n",
    "measure_df.iloc[:,2] = 'purity, silhouette_score'\n",
    "measure_df.iloc[:,3] = 'purity, silhouette_score'\n",
    "measure_df.iloc[:,4] = 'purity, silhouette_score'\n",
    "                        \n",
    "df = AutoML(scaler_list, encoder_list, model_list,hyperparmeter_df, dataset_non_target, dataset_target, categorical_attr_list, measure_df)\n",
    "result_sorted_1 = df.sort_values(by = ['knee_method'], ascending = False)\n",
    "print('TOP 5 knee_method INFO------------------------------------')\n",
    "print(result_sorted_1.head(5))\n",
    "result_sorted_2 = df.sort_values(by = ['silhouette_score'], ascending = False)\n",
    "print('TOP 5 silhouette_score INFO------------------------------------')\n",
    "print(result_sorted_2.head(5))\n",
    "result_sorted_3 = df.sort_values(by = ['purity'], ascending = True)\n",
    "print('TOP 5 purity INFO------------------------------------')\n",
    "print(result_sorted_3.head(5))\n",
    " = df.sort_values(by = ['dist'], ascending = True)\n",
    "print('TOP 5 dist INFO------------------------------------')\n",
    "print(result_sorted_4.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
